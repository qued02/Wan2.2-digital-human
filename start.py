print("å¯åŠ¨ä¸­...")
import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
RUNDIR = os.path.dirname(os.path.abspath(__file__)) + "/"
# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["HUGGINGFACE_HUB_BASE_URL"] = "https://hf-mirror.com"
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

os.environ["TEMP"] = f"{RUNDIR}temp"
os.environ["TMP"] = f"{RUNDIR}temp"
os.environ["TORCH_HOME"] = f"{RUNDIR}torch"
os.environ["MODELSCOPE_CACHE"] = f"{RUNDIR}modelscope"

os.environ["NLTK_DATA"] = f"{RUNDIR}nltk_data"

import torch
from PIL import Image
from diffsynth import save_video, VideoData, load_state_dict
from diffsynth.pipelines.wan_video_new import WanVideoPipeline, ModelConfig
from modelscope import snapshot_download
from pathlib import Path
import cv2

# è®¾ç½®PATH
python_paths = [
    f"{RUNDIR}py310",
    f"{RUNDIR}py310/Scripts",
    os.environ.get("PATH", "")
]
os.environ["PATH"] = ";".join(python_paths)

# è®¾ç½®PYTHONPATH
python_paths = [
    f"{RUNDIR}py310",
    os.environ.get("PYTHONPATH", "")
]
os.environ["PYTHONPATH"] = ";".join(python_paths)

import sys
import uuid
import shutil
import time
import subprocess
import argparse
import tempfile
import gradio as gr
from pathlib import Path
from datetime import datetime
from moviepy import *

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))



#åˆæˆæœ€ç»ˆè§†é¢‘
def DiffSynth_generate_video(mode,inputimage,animateposevideo,animatefacevideo,animate_inpaint_video,animate_mask_video,num_frames,height,width,inference_step,final_video_path,final_fps):
        
    pipe = WanVideoPipeline.from_pretrained(
        torch_dtype=torch.bfloat16,
        device="cuda",
        model_configs=[
            ModelConfig(path=[
            "./Wan2.2-Animate-14B/diffusion_pytorch_model-00001-of-00004.safetensors",
            "./Wan2.2-Animate-14B/diffusion_pytorch_model-00002-of-00004.safetensors",
            "./Wan2.2-Animate-14B/diffusion_pytorch_model-00003-of-00004.safetensors",
            "./Wan2.2-Animate-14B/diffusion_pytorch_model-00004-of-00004.safetensors",            
            ], offload_device="cpu"),
            ModelConfig(path="./Wan2.2-Animate-14B/models_t5_umt5-xxl-enc-bf16.pth", offload_device="cpu"),
            ModelConfig(path="./Wan2.2-Animate-14B/Wan2.1_VAE.pth", offload_device="cpu"),
            ModelConfig(path="./Wan2.2-Animate-14B/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth", offload_device="cpu"),
            
        ],
    )
    pipe.enable_vram_management()
    a=num_frames-4
    input_image = Image.open(inputimage)
    if mode=="animate":
        # Animate
        
        #animate_pose_video = VideoData("data/examples/wan/animate/animate_pose_video.mp4").raw_data()[:81-4]
        #animate_face_video = VideoData("data/examples/wan/animate/animate_face_video.mp4").raw_data()[:81-4]
        
        animate_pose_video = VideoData(animateposevideo).raw_data()[:a]
        animate_face_video = VideoData(animatefacevideo).raw_data()[:a]
        video = pipe(
            prompt="è§†é¢‘ä¸­çš„äººåœ¨åšåŠ¨ä½œ",
            seed=0, tiled=True,
            input_image=input_image,
            animate_pose_video=animate_pose_video,
            animate_face_video=animate_face_video,
            num_frames=num_frames, height=height, width=width,
            num_inference_steps=inference_step, cfg_scale=1,
        )
        save_video(video,final_video_path, fps=final_fps, quality=5)              
    
    else:
        # Replace äººç‰©æ›¿æ¢æ¨¡å¼
        lora_state_dict = load_state_dict("./Wan2.2-Animate-14B/relighting_lora.ckpt", torch_dtype=torch.float32, device="cuda")["state_dict"]
        pipe.load_lora(pipe.dit, state_dict=lora_state_dict)
        #input_image = Image.open(inputimage)
        animate_pose_video = VideoData(animateposevideo).raw_data()[:a]
        animate_face_video = VideoData(animatefacevideo).raw_data()[:a]
        animate_inpaint_video = VideoData(animate_inpaint_video).raw_data()[:a]
        animate_mask_video = VideoData(animate_mask_video).raw_data()[:a]
        video = pipe(
            prompt="è§†é¢‘ä¸­çš„äººåœ¨åšåŠ¨ä½œ",
            seed=0, tiled=True,
            input_image=input_image,
            animate_pose_video=animate_pose_video,
            animate_face_video=animate_face_video,
            animate_inpaint_video=animate_inpaint_video,
            animate_mask_video=animate_mask_video,
            num_frames=num_frames, height=height, width=width,
            num_inference_steps=inference_step, cfg_scale=1,
        )
        save_video(video, final_video_path, fps=final_fps, quality=5)
        
        
    # æŸ¥æ‰¾ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶
    file_path = Path(final_video_path)
    if file_path.is_file():
        return True, final_video_path
    else:
        return False, "Generated video not found"
        
#è·å–æœ‰æ•ˆå¸§æ•°å­—
def adjust_fps_number(n):
    return n if n % 4 == 1 else n - (n % 4 - 1) % 4
    
    
class WanAnimateApp:
    def __init__(self, ckpt_dir="./Wan2.2-Animate-14B"):
        self.ckpt_dir = ckpt_dir
        self.process_checkpoint = os.path.join(ckpt_dir, "process_checkpoint")
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•ï¼Œè¾“å‡ºç›®å½•
        os.makedirs("./output/process_results/animate", exist_ok=True)
        os.makedirs("./output/process_results/replace", exist_ok=True)
        
    def preprocess_data(self, video_path, refer_path, mode, resolution_area, 
                       use_flux=False, iterations=3, k=7, w_len=1, h_len=1):
        """
        é¢„å¤„ç†æ•°æ®
        """
        try:
            # æ ¹æ®æ¨¡å¼ç¡®å®šä¿å­˜è·¯å¾„
            if mode == "animate":
                save_path = "./output/process_results/animate"
                replace_flag = False
                retarget_flag = True
            else:  # replace mode
                save_path = "./output/process_results/replace"
                replace_flag = True
                retarget_flag = False
            
            # æ„å»ºé¢„å¤„ç†å‘½ä»¤
            cmd = [
                "python", "./wan/modules/animate/preprocess/preprocess_data.py",
                "--ckpt_path", self.process_checkpoint,
                "--video_path", video_path,
                "--refer_path", refer_path,
                "--save_path", save_path,
                "--resolution_area", str(resolution_area[0]), str(resolution_area[1])
            ]
            
            # æ·»åŠ æ¨¡å¼ç‰¹å®šçš„å‚æ•°
            if mode == "animate":
                if retarget_flag:
                    cmd.append("--retarget_flag")
                if use_flux:
                    cmd.append("--use_flux")
            else:  # replace mode
                if replace_flag:
                    cmd.append("--replace_flag")
                cmd.extend(["--iterations", str(iterations)])
                cmd.extend(["--k", str(k)])
                cmd.extend(["--w_len", str(w_len)])
                cmd.extend(["--h_len", str(h_len)])
            
            # è¿è¡Œé¢„å¤„ç†
            #print(f"Running preprocessing command: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                return False, f"Preprocessing failed: {result.stderr}"
            
            return True, save_path
            
        except Exception as e:
            return False, f"Preprocessing error: {str(e)}"    
  
    
    def process_and_generate(self, ref_img, video, mode, resolution_width, resolution_height,
                           use_flux, iterations, k, w_len, h_len, refert_num, use_relighting_lora):
        """
        å®Œæ•´çš„å¤„ç†æµç¨‹ï¼šé¢„å¤„ç† + ç”Ÿæˆ
        """
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as tmp_ref:
                # ä¿å­˜å‚è€ƒå›¾åƒ
                if hasattr(ref_img, 'save'):
                    ref_img.save(tmp_ref.name)
                else:
                    shutil.copy(ref_img, tmp_ref.name)
                ref_path = tmp_ref.name
            
            with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp_vid:
                # ä¿å­˜è§†é¢‘
                shutil.copy(video, tmp_vid.name)
                vid_path = tmp_vid.name
            
            # æ­¥éª¤1: é¢„å¤„ç†
            yield "å¼€å§‹é¢„å¤„ç†...", None
            resolution_area = [resolution_width, resolution_height]
            success, preprocess_result = self.preprocess_data(
                vid_path, ref_path, mode, resolution_area, use_flux, 
                iterations, k, w_len, h_len
            )
            
            if not success:
                yield f"é¢„å¤„ç†å¤±è´¥: {preprocess_result}", None
                return
            
            yield "ç¬¬ä¸€æ­¥é¢„å¤„ç†å®Œæˆï¼Œå¼€å§‹æå–é©±åŠ¨è§†é¢‘å‚æ•°...", None
            
            # æ­¥éª¤2: ç”Ÿæˆè§†é¢‘
            #ç¡®å®šé¢„å¤„ç†åçš„æ–‡ä»¶è·¯å¾„
            inputimage=preprocess_result+"/src_ref.png"
            animateposevideo=preprocess_result+"/src_pose.mp4"
            animatefacevideo=preprocess_result+"/src_face.mp4"
            animate_inpaint_video=preprocess_result+"/src_bg.mp4"
            animate_mask_video=preprocess_result+"/src_mask.mp4"
            #è·å–æœ€ç»ˆåˆæˆçš„è§†é¢‘å¸§æ•°
            cap = cv2.VideoCapture(animateposevideo)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))            
            num_frames=adjust_fps_number(frame_count)
            height=resolution_height
            width=resolution_width
            inference_step=20
            current_time = datetime.now().strftime("%Y%m%d%H%M%S")
            final_video_path="./output/"+current_time+".mp4"
            #è·å–è§†é¢‘å¸§ç‡
            final_fps=cap.get(cv2.CAP_PROP_FPS)
            yield "é©±åŠ¨è§†é¢‘å‚æ•°æå–å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆæ¨¡ä»¿è§†é¢‘...", None
            success, generation_result = DiffSynth_generate_video(mode,inputimage,animateposevideo,animatefacevideo,animate_inpaint_video,animate_mask_video,num_frames,height,width,inference_step,final_video_path,final_fps)
            #DiffSynth_generate_video(mode,inputimage,animateposevideo,animatefacevideo,animate_inpaint_video,animate_mask_video,num_frames,height,width,inference_step,final_video_path,final_fps):
            # success, generation_result = self.generate_video(
            #     preprocess_result, mode, refert_num, use_relighting_lora
            # )
            
            if not success:
                yield f"ç”Ÿæˆå¤±è´¥: {generation_result}", None
                return
            yield "æ¨¡ä»¿è§†é¢‘ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹å¤„ç†éŸ³é¢‘!", None
            #æå–åŸé©±åŠ¨è§†é¢‘ä¸­çš„éŸ³é¢‘
            #base_name = os.path.splitext(vid_path)[0]
            audio_output_path ="./output/"+ current_time+"_audio.mp3"
            
            # åŠ è½½è§†é¢‘å¹¶æå–éŸ³é¢‘
            video = VideoFileClip(vid_path)
            audio = video.audio
            
            # ä¿å­˜éŸ³é¢‘
            audio.write_audiofile(audio_output_path)
            
            # é‡Šæ”¾èµ„æº
            video.close()
            audio.close()

            if os.path.exists(audio_output_path):
                output_path = "./output/"+current_time+"_final.mp4"
                
                # åŠ è½½è§†é¢‘å’ŒéŸ³é¢‘
                video = VideoFileClip(generation_result)
                audio = AudioFileClip(audio_output_path)
                            
                # è®¾ç½®è§†é¢‘çš„éŸ³é¢‘      
                new_audioclip = CompositeAudioClip([audio])
                video.audio = new_audioclip 
                
                # ä¿å­˜åˆå¹¶åçš„è§†é¢‘
                video.write_videofile(
                    output_path,
                    codec='libx264',
                    audio_codec='aac'
                )
                generation_result=output_path        
        
                
            
            yield "è§†é¢‘ç”Ÿæˆå®Œæˆ!", generation_result
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(ref_path)
            os.unlink(vid_path)
            
        except Exception as e:
            yield f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", None

def create_gradio_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    # åˆå§‹åŒ–åº”ç”¨
    app = WanAnimateApp()
    
    with gr.Blocks(title="Wan2.2-Animate è§†é¢‘ç”Ÿæˆå·¥å…·", theme=gr.themes.Ocean()) as demo:
        gr.Markdown("# ğŸ¬ Wan2.2-Animate-14B åŠ¨ä½œæ¨¡ä»¿åŠäººç‰©æ›¿æ¢è§†é¢‘ç”Ÿæˆå·¥å…·")
        gr.Markdown("### æ•°å­—äºº made by Shihan Qu")
        with gr.Tabs():
            with gr.TabItem("åŠ¨ç”»æ¨¡å¼"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### åŠ¨ç”»æ¨¡å¼é…ç½®")
                        animate_ref_img = gr.Image(
                            label="å‚è€ƒå›¾åƒ (Reference Image)",
                            type="filepath",
                            sources=["upload"],
                            height=200
                        )
                        animate_video = gr.Video(
                            label="é©±åŠ¨è§†é¢‘ (Driving Video)",
                            sources=["upload"],
                            height=200
                        )
                        
                        with gr.Row():
                            animate_res_width = gr.Number(
                                label="åˆ†è¾¨ç‡å®½åº¦", value=1280, precision=0
                            )
                            animate_res_height = gr.Number(
                                label="åˆ†è¾¨ç‡é«˜åº¦", value=720, precision=0
                            )
                        
                        animate_use_flux = gr.Checkbox(
                            label="ä½¿ç”¨FLUXå›¾åƒç¼–è¾‘",
                            value=False,
                            info="æ¨èåœ¨å‚è€ƒå›¾åƒæˆ–é©±åŠ¨è§†é¢‘ç¬¬ä¸€å¸§ä¸æ˜¯æ ‡å‡†æ­£é¢å§¿åŠ¿æ—¶ä½¿ç”¨"
                        )
                        
                        animate_refert_num = gr.Number(
                            label="æ—¶åºå¼•å¯¼å¸§æ•°", value=1, precision=0,
                            info="ç”¨äºæ—¶åºå¼•å¯¼çš„å¸§æ•°ï¼Œæ¨è1æˆ–5"
                        )
                        
                        animate_run_btn = gr.Button("ğŸš€ å¼€å§‹ç”ŸæˆåŠ¨ç”»", variant="primary")
                    
                    with gr.Column():
                        gr.Markdown("### è¾“å‡ºç»“æœ")
                        animate_output_video = gr.Video(label="ç”Ÿæˆçš„åŠ¨ç”»è§†é¢‘")
                        animate_status = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False,lines=5)
            
            with gr.TabItem("æ›¿æ¢æ¨¡å¼"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### æ›¿æ¢æ¨¡å¼é…ç½®")
                        replace_ref_img = gr.Image(
                            label="å‚è€ƒå›¾åƒ (Reference Image)",
                            type="filepath",
                            sources=["upload"],
                            height=200
                        )
                        replace_video = gr.Video(
                            label="æºè§†é¢‘ (Source Video)",
                            sources=["upload"],
                            height=200
                        )
                        
                        with gr.Row():
                            replace_res_width = gr.Number(
                                label="åˆ†è¾¨ç‡å®½åº¦", value=1280, precision=0
                            )
                            replace_res_height = gr.Number(
                                label="åˆ†è¾¨ç‡é«˜åº¦", value=720, precision=0
                            )
                        
                        gr.Markdown("#### æ©ç ç­–ç•¥å‚æ•°")
                        with gr.Row():
                            replace_iterations = gr.Number(
                                label="è¿­ä»£æ¬¡æ•°", value=3, precision=0,
                                info="æ©ç è†¨èƒ€çš„è¿­ä»£æ¬¡æ•°"
                            )
                            replace_k = gr.Number(
                                label="æ ¸å¤§å°", value=7, precision=0,
                                info="æ©ç è†¨èƒ€çš„æ ¸å¤§å°"
                            )
                        
                        with gr.Row():
                            replace_w_len = gr.Number(
                                label="Wç»´åº¦ç»†åˆ†", value=1, precision=0,
                                info="æ²¿Wç»´åº¦çš„ç»†åˆ†æ•°é‡ï¼Œå€¼è¶Šé«˜è½®å»“è¶Šè¯¦ç»†"
                            )
                            replace_h_len = gr.Number(
                                label="Hç»´åº¦ç»†åˆ†", value=1, precision=0,
                                info="æ²¿Hç»´åº¦çš„ç»†åˆ†æ•°é‡ï¼Œå€¼è¶Šé«˜è½®å»“è¶Šè¯¦ç»†"
                            )
                        
                        replace_use_relighting_lora = gr.Checkbox(
                            label="ä½¿ç”¨é‡å…‰ç…§LoRA",
                            value=False
                        )
                        
                        replace_refert_num = gr.Number(
                            label="æ—¶åºå¼•å¯¼å¸§æ•°", value=1, precision=0,
                            info="ç”¨äºæ—¶åºå¼•å¯¼çš„å¸§æ•°ï¼Œæ¨è1æˆ–5"
                        )
                        
                        replace_run_btn = gr.Button("ğŸš€ å¼€å§‹è§’è‰²æ›¿æ¢", variant="primary")
                    
                    with gr.Column():
                        gr.Markdown("### è¾“å‡ºç»“æœ")
                        replace_output_video = gr.Video(label="æ›¿æ¢åçš„è§†é¢‘")
                        replace_status = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False,lines=5)
        
        # åŠ¨ç”»æ¨¡å¼çš„äº‹ä»¶å¤„ç†
        animate_run_btn.click(
            fn=app.process_and_generate,
            inputs=[
                animate_ref_img,
                animate_video,
                gr.Text("animate", visible=False),
                animate_res_width,
                animate_res_height,
                animate_use_flux,
                gr.Number(3, visible=False),  # iterations
                gr.Number(7, visible=False),  # k
                gr.Number(1, visible=False),  # w_len
                gr.Number(1, visible=False),  # h_len
                animate_refert_num,
                gr.Checkbox(False, visible=False),  # use_relighting_lora
            ],
            outputs=[animate_status, animate_output_video]
        )
        
        # æ›¿æ¢æ¨¡å¼çš„äº‹ä»¶å¤„ç†
        replace_run_btn.click(
            fn=app.process_and_generate,
            inputs=[
                replace_ref_img,
                replace_video,
                gr.Text("replace", visible=False),
                replace_res_width,
                replace_res_height,
                gr.Checkbox(False, visible=False),  # use_flux
                replace_iterations,
                replace_k,
                replace_w_len,
                replace_h_len,
                replace_refert_num,
                replace_use_relighting_lora,
            ],
            outputs=[replace_status, replace_output_video]
        )                    
    return demo

#def start_app():
def main():
    """å¯åŠ¨åº”ç”¨"""
    parser = argparse.ArgumentParser(description="Wan2.2-Animate Gradioåº”ç”¨")
    parser.add_argument("--server_name", type=str, default="0.0.0.0",
                       help="æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--server_port", type=int, default=7861,
                       help="æœåŠ¡å™¨ç«¯å£")
    
    args = parser.parse_args()
    
    
    # åˆ›å»ºå¹¶å¯åŠ¨Gradioåº”ç”¨
    demo = create_gradio_interface()
    
    demo.launch(
        server_name=args.server_name,
        server_port=args.server_port,
        inbrowser=True
    )

if __name__ == "__main__":
    try:
    #start_app()
        main()
    except Exception as e:
        with open("error.log", "w") as f:
            f.write(traceback.format_exc())
        print("ç¨‹åºå´©æºƒï¼Œé”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°error.log")
        # ä¹Ÿå¯ä»¥é€‰æ‹©å°†é”™è¯¯æ‰“å°å‡ºæ¥
        traceback.print_exc(file=sys.stdout)
        sys.exit(1)